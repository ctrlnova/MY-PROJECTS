{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFxOaowXyXY4",
        "outputId": "5a73bf71-c18a-400a-ed7f-3708b3900e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "its paragraph wise"
      ],
      "metadata": {
        "id": "yHKxnIwf5RnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define file paths for the human-readable and MTL text files\n",
        "human_readable_file = '/content/novel_chapters/chapter_49.txt'\n",
        "mtl_file = '/content/drive/MyDrive/pratice models/NEW MTL TO HRL/MTL/49.docx.txt'\n",
        "\n",
        "# Read the content of both files into lists\n",
        "with open(human_readable_file, 'r', encoding='utf-8') as hr_file:\n",
        "    human_readable_lines = hr_file.read().splitlines()\n",
        "\n",
        "with open(mtl_file, 'r', encoding='utf-8') as mtl_file:\n",
        "    mtl_lines = mtl_file.read().splitlines()\n",
        "\n",
        "# Create a TF-IDF vectorizer to convert text to numerical vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "human_readable_vectors = vectorizer.fit_transform(human_readable_lines)\n",
        "\n",
        "# Calculate cosine similarity line by line and store the most similar pairs\n",
        "dataset = []\n",
        "for hr_line in human_readable_lines:\n",
        "    hr_vector = vectorizer.transform([hr_line])\n",
        "    max_similarity = -1  # Initialize with a value lower than any possible similarity\n",
        "    max_similarity_index = -1\n",
        "    for i, mtl_line in enumerate(mtl_lines):\n",
        "        mtl_vector = vectorizer.transform([mtl_line])\n",
        "        similarity = cosine_similarity(hr_vector, mtl_vector)[0][0]\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_index = i\n",
        "    dataset.append({'MTL_Text': mtl_lines[max_similarity_index], 'Human_Readable_Text': hr_line})\n",
        "\n",
        "# Convert the dataset list of dictionaries to a DataFrame\n",
        "dataset_df = pd.DataFrame(dataset)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "dataset_df.to_csv('mtl_to_human_dataset.csv', index=False)\n",
        "\n",
        "# Print the first few rows of the dataset to check\n",
        "print(dataset_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XokjkTkC0qW",
        "outputId": "a5803661-aa95-4dd7-8d86-69d6070b16e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            MTL_Text  \\\n",
            "0   ﻿Star Odyssey - 49. Chapter 49 The Strange Stone   \n",
            "1   ﻿Star Odyssey - 49. Chapter 49 The Strange Stone   \n",
            "2  In the capital, Ras’ heart was ashamed, even M...   \n",
            "3  Mono was very famous among the students in the...   \n",
            "4  Lu Yin had the final say, “Seventeen, the fugi...   \n",
            "\n",
            "                                 Human_Readable_Text  \n",
            "0                         ﻿Chapter 49: Strange Stone  \n",
            "1                                                     \n",
            "2  Back in the capital, Raas had pretty much give...  \n",
            "3  Munoor was famous amongst the trainees, and ne...  \n",
            "4  Not long later, a few more students were captu...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "its for processing the text"
      ],
      "metadata": {
        "id": "pQIQF0NT5JmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/all.csv')\n",
        "\n",
        "# Function to preprocess text pairs without changing pairings\n",
        "def preprocess_text_pair(pair):\n",
        "    mtl_text, human_text = pair\n",
        "    # Convert text to lowercase\n",
        "    mtl_text = mtl_text.lower()\n",
        "    human_text = human_text.lower()\n",
        "    # Remove punctuation and non-alphanumeric characters\n",
        "    mtl_text = re.sub(r'[^a-zA-Z0-9\\s]', '', mtl_text)\n",
        "    human_text = re.sub(r'[^a-zA-Z0-9\\s]', '', human_text)\n",
        "    # Tokenize text\n",
        "    mtl_tokens = word_tokenize(mtl_text)\n",
        "    human_tokens = word_tokenize(human_text)\n",
        "    # Remove repeating words\n",
        "    mtl_tokens = list(dict.fromkeys(mtl_tokens))\n",
        "    human_tokens = list(dict.fromkeys(human_tokens))\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    mtl_filtered_tokens = [word for word in mtl_tokens if word not in stop_words]\n",
        "    human_filtered_tokens = [word for word in human_tokens if word not in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    mtl_lemmatized_tokens = [lemmatizer.lemmatize(token) for token in mtl_filtered_tokens]\n",
        "    human_lemmatized_tokens = [lemmatizer.lemmatize(token) for token in human_filtered_tokens]\n",
        "    # Join tokens back into text\n",
        "    mtl_preprocessed_text = ' '.join(mtl_lemmatized_tokens)\n",
        "    human_preprocessed_text = ' '.join(human_lemmatized_tokens)\n",
        "    return mtl_preprocessed_text, human_preprocessed_text\n",
        "\n",
        "# Apply preprocessing to each text pair\n",
        "df['MTL_Text_Preprocessed'], df['Human_Readable_Text_Preprocessed'] = zip(*df[['MTL_Text', 'Human_Readable_Text']].apply(preprocess_text_pair, axis=1))\n",
        "\n",
        "# Save the preprocessed data to a new CSV file\n",
        "df.to_csv('preprocessed_texts.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1727996e-9ec2-4d7c-80b7-abaeb01f28a4",
        "id": "Fi-nLE-GxIay"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "updated of the above\n"
      ],
      "metadata": {
        "id": "3Bld91CYxIa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "is sentence wise simiarity applied"
      ],
      "metadata": {
        "id": "gqChpciexIa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('', names=['MTL_Text', 'Human_Readable_Text'])\n",
        "\n",
        "# Filter out rows where one column is empty\n",
        "df = df.dropna()\n",
        "\n",
        "# Write the filtered data to a new CSV file\n",
        "df.to_csv('filtered_file.csv', index=False)\n"
      ],
      "metadata": {
        "id": "hnhwokmYC0sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/all.csv')\n",
        "\n",
        "# Function to preprocess text pairs without changing pairings\n",
        "def preprocess_text_pair(pair):\n",
        "    mtl_text, human_text = pair\n",
        "    # Convert text to lowercase\n",
        "    mtl_text = mtl_text.lower()\n",
        "    human_text = human_text.lower()\n",
        "    # Remove punctuation and non-alphanumeric characters\n",
        "    mtl_text = re.sub(r'[^a-zA-Z0-9\\s]', '', mtl_text)\n",
        "    human_text = re.sub(r'[^a-zA-Z0-9\\s]', '', human_text)\n",
        "    # Tokenize text\n",
        "    mtl_tokens = word_tokenize(mtl_text)\n",
        "    human_tokens = word_tokenize(human_text)\n",
        "    # Remove repeating words\n",
        "    mtl_tokens = list(dict.fromkeys(mtl_tokens))\n",
        "    human_tokens = list(dict.fromkeys(human_tokens))\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    mtl_filtered_tokens = [word for word in mtl_tokens if word not in stop_words]\n",
        "    human_filtered_tokens = [word for word in human_tokens if word not in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    mtl_lemmatized_tokens = [lemmatizer.lemmatize(token) for token in mtl_filtered_tokens]\n",
        "    human_lemmatized_tokens = [lemmatizer.lemmatize(token) for token in human_filtered_tokens]\n",
        "    # Join tokens back into text\n",
        "    mtl_preprocessed_text = ' '.join(mtl_lemmatized_tokens)\n",
        "    human_preprocessed_text = ' '.join(human_lemmatized_tokens)\n",
        "    return mtl_preprocessed_text, human_preprocessed_text\n",
        "\n",
        "# Apply preprocessing to each text pair\n",
        "df['MTL_Text_Preprocessed'], df['Human_Readable_Text_Preprocessed'] = zip(*df[['MTL_Text', 'Human_Readable_Text']].apply(preprocess_text_pair, axis=1))\n",
        "\n",
        "# Save the preprocessed data to a new CSV file\n",
        "df.to_csv('preprocessed_texts.csv', index=False)\n"
      ],
      "metadata": {
        "id": "FPF0Fjr5C0vN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1727996e-9ec2-4d7c-80b7-abaeb01f28a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "updated of the above\n"
      ],
      "metadata": {
        "id": "SOR9WlC6JzMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "above is good\n",
        "nest is of sentence wise"
      ],
      "metadata": {
        "id": "_KPa6B90yJk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of URLs for different chapters\n",
        "chapter_urls = [\n",
        "\n",
        "\n",
        "\n",
        "\"https://www.novels.pl/novel/160/1/Chapter-1-Lu-Yin.html\",\n",
        "\"https://www.novels.pl/novel/160/2/Chapter-2-Camp-of-the-Seven-Sages.html\",\n",
        "\"https://www.novels.pl/novel/160/3/Chapter-3-Arrival.html\",\n",
        "\"https://www.novels.pl/novel/160/4/Chapter-4-Formcast-Model.html\",\n",
        "\"https://www.novels.pl/novel/160/5/Chapter-5-Cultivation.html\",\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Sentences to remove\n",
        "sentences_to_remove = [\n",
        "    \"This chapter is updated by Novels.pl\",\n",
        "    \"PreviousNext\",\n",
        "    \"You can also listen on bestnovel.org\",\n",
        "    \"I created a game for Android Idle Ninja Empire , I could use a little support in promoting it, just download it and play for a while. Thank you in advance.\"\n",
        "    # Add more sentences to remove as needed\n",
        "]\n",
        "\n",
        "# Folder to save the chapters\n",
        "folder_name = 'novel_chapters'\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Loop through the chapter URLs\n",
        "for idx, url in enumerate(chapter_urls, start=1):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the element containing the chapter content\n",
        "    chapter_content = soup.find('div', class_='panel-body article')\n",
        "\n",
        "    if chapter_content:\n",
        "        # Extract the text from the element\n",
        "        chapter_text = chapter_content.get_text()\n",
        "\n",
        "        # Remove unwanted sentences\n",
        "        for sentence in sentences_to_remove:\n",
        "            chapter_text = chapter_text.replace(sentence, '')\n",
        "\n",
        "        # Remove the last two lines\n",
        "        chapter_lines = chapter_text.split('\\n')\n",
        "        if len(chapter_lines) > 2:\n",
        "            chapter_text = '\\n'.join(chapter_lines[:-2])\n",
        "\n",
        "        # File path for the chapter\n",
        "        file_path = os.path.join(folder_name, f'chapter_{idx}.txt')\n",
        "\n",
        "        # Write the modified chapter text to the file\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(chapter_text)\n",
        "    else:\n",
        "        print(\"Chapter content not found for:\", url)\n",
        "\n",
        "print(f\"Chapters saved to the '{folder_name}' folder\")\n"
      ],
      "metadata": {
        "id": "w33U5BQnC05m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdffe5c2-d8d7-4902-d08a-27143ea8377a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapters saved to the 'novel_chapters' folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fqYUgdStkPQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}